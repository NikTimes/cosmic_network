
% ==========================
% main.tex — Scientific paper template
% Features:
%  • BibTeX (natbib) with a self-contained references.bib via filecontents*
%  • TikZ neural network diagram example
%  • Tables (booktabs + siunitx), figures, math (amsmath), theorems
%  • Code listings (listings; optional minted)
%  • PGFPlots example figure
%  • Clever cross-references + hyperlinks
% ==========================


\documentclass[11pt,a4paper]{article}

% --- Encoding, fonts, microtypography
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{microtype}

% --- Page geometry
\usepackage[a4paper,margin=1in]{geometry}

% --- Math and theorem environments
\usepackage{amsmath,amssymb,amsfonts,mathtools}
\usepackage{amsthm}
\numberwithin{equation}{section}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

% --- Figures, subfigures, graphics
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}

% --- Tables
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{siunitx}
\sisetup{detect-all=true, round-mode=places, round-precision=3}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}

% --- Code listings (simple, portable). For minted, see below.
\usepackage{listings}
\lstset{basicstyle=\ttfamily\small,columns=fullflexible,breaklines=true,frame=single,numbers=left,numberstyle=\tiny}
% Optional: \usepackage{minted} (requires -shell-escape and Pygments)

% --- Hyperlinks + Clever references
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue}
\usepackage[nameinlink,noabbrev]{cleveref}

% --- Bibliography (BibTeX via natbib)
\usepackage[numbers,sort&compress]{natbib}
\bibliographystyle{unsrtnat}

% --- Algorithms (choose one of these families)
% \usepackage{algorithm}
% \usepackage{algpseudocode}
% OR this convenient single-package alternative:
\usepackage[ruled,vlined]{algorithm2e}

% --- TikZ + PGFPlots for neural network diagrams and plots
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,calc,fit}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

% Styles for NN drawings
\tikzset{
  neuron/.style={circle,draw,minimum size=15pt,inner sep=0,outer sep=0},
  input neuron/.style={neuron,fill=gray!10},
  hidden neuron/.style={neuron,fill=gray!20},
  output neuron/.style={neuron,fill=gray!10},
  connect/.style={-Latex}
}

% --- Title metadata
\title{Your Awesome Title: A Concise Scientific Paper Template}
\author{First Author$^{1}$ \and Second Author$^{2}$ \\
  \small $^{1}$Affiliation One \\ \small $^{2}$Affiliation Two}
\date{\today}

% --- Convenience macros (edit or remove as needed)
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}

\begin{document}
\maketitle

\begin{abstract}
We present a minimal, modern \LaTeX{} template for scientific papers: Bib\TeX{} with \texttt{natbib}, neural network diagrams via TikZ, math and theorems, tables with \texttt{booktabs}/\texttt{siunitx}, code listings, and PGFPlots figures.\footnote{Repository-ready: drop in your own \texttt{references.bib} or keep the auto-generated one.}
\end{abstract}

\section{Introduction}
Cite seminal work with author-year commands, e.g., \citet{lecun2015deep} and grouped numeric-style citations \citep{kingma2014adam,he2016deep}. See \cref{sec:methods} for methods and \cref{sec:results} for results.

\subsection{Contributions}
\begin{itemize}
  \item A complete paper skeleton with sensible defaults.
  \item An inline TikZ example for a feed-forward neural network diagram (\cref{fig:nn}).
  \item Ready-to-use table and figure examples (\cref{tab:results,fig:plot}).
\end{itemize}

\section{Background}


The purpose of this paper is not only to showcase a surrogate model for all densities in the $\Lambda$CDM model but also to shine a light on the role of machine learning in Computational Physics. For that reason, in the background section we review both the physics and computational challenges of deriving the angular power spectrum, as well as the mathematical principles underlying neural networks, with particular emphasis on backpropagation. 

In what follows, we begin with a review of the physical origin and relevance of the angular power spectrum, followed by an overview of the tools used to simulate it. These being Bolztmann solvers such as CAMB and CLASS. 
We then proceed by motivating surrogate models and their usefulness in cosmology before diving in the mathematics of neural networks and their training via gradient descent. 
Finally, we outline the role of principal component analysis as a tool for dimensionality reduction in the context of building a surrogate to emulate the CMB power spectrum.


\subsection{Angular Power Spectrum}
\label{sec:aps}

\subsubsection{Physical origin of the CMB}

The CMB is the earliest possible electromagnetic image that we can currently observe. 
Prior to its release, the universe was a hot, dense plasma in which all components of the standard model were in thermal equilibrium with each other, including matter and radiation.  

In this tightly coupled state, photons interacted continuously with the surrounding plasma, primarily through Thomson scattering off free electrons
As a result, radiation could not propagate freely which made direct observation impossible.

To model the expanding universe during this epoch we use the \textbf{flat Friedmann–Lemaître–Robertson–Walker (FLRW) metric}

\begin{equation}
ds^2 = -c^2dt^2 + a^2(t)\left[dr^2 + r^2(d\theta^2 + \sin^2\theta d\phi^2) \right],
\end{equation}
Where $a(t)$ is the \textbf{scale factor} driving the expansion of the universe. 

As $a(t)$ grows the universe cools according to $T \propto 1/a(t)$. This cooling not only suppresses the average energy of photons, but also enables the formation of stable neutral hydrogen atoms. As a result, processes such as Thomson scattering ceased to be efficient, allowing photons to decouple from matter. These photons have since propagated freely across the universe, forming the Cosmic Microwave Background.
\subsubsection{Angular Power Spectrum}
Observationally, measurements of the CMB reveal that while the observed photon spectrum follows a nearly perfect blackbody with constant average temperature, a detailed analysis using the Planck distribution reveals tiny anisotropies. 

We can define the CMB temperature anisotropies as a function on the celestial sphere: 
\begin{equation}
  \Theta(\hat n) = \frac{\Delta T(\hat n)}{T}
\end{equation}

where $\hat n$ denotes a direction on the sky. Since the sky is a two-sphere, we can express this function in spherical harmonics as:
\begin{equation}
  \Theta(\hat n) = \sum_{\ell=0}^{\infty} \sum_{m = -\ell}^\ell a_{\ell m}Y_{\ell m}(\hat n).
\end{equation}

Because the CMB is well described as a nearly Gaussian random field, all of its statistical information is contained in its \textbf{two-point correlation} function:
\begin{equation}
  C(\theta) = \langle\Delta T(\hat n)\Delta T(\hat n')\rangle.
\end{equation}

Statistical isotropy then leads to: 
\begin{equation}
  \langle a_{\ell m} a_{\ell' m'}^*\rangle = C_{\ell} \delta_{\ell \ell'} \delta_{mm'}.
\end{equation}

Therefore the set of coefficients driving the spherical harmonic decomposition of $\Theta(\hat n)$ is completely described by the set of coefficients $\{C_\ell\}$, which make up the \textbf{angular power spectrum}. 

Currently our most precise measurement of the angular power spectrum was carried out by Planck 2018. Any viable cosmological theory must be able to reproduce the main features of that spectrum within uncertainty.  

% FIGURE placeholder: Planck 2018 angular power spectrum

Planck finds that the six-parameter $\Lambda$CDM model provides an excellent fit to the angular power spectrum. $\Lambda$CDM is characterized by the fractional energy densities of the main components of the Universe—baryons ($b$), dark matter ($\text{cdm}$), photons ($\gamma$), relativistic neutrinos ($\text{ur}$), dark energy ($\Lambda$), and curvature ($k$)—denoted by $\Omega_i$. These follow the Friedmann constraint:
\begin{equation}
\Omega_{\Lambda} + \Omega_k + \Omega_\text{cdm} + \Omega_b+ \Omega_\text{ur} + \Omega_\gamma = 1.
\end{equation}

While the Planck analysis is performed in terms of the following independent parameters $(\Omega_b h^2,\, \Omega_c h^2,\, 100\,\theta_s,\, \tau,\, n_s,\, \ln(10^{10}A_s))$,
these uniquely determine the values of the density fractions above. Varying these $\Omega_i$ allows us to study how each component of the cosmic fabric affects the CMB power spectrum. 

\subsection{Boltzmann Solvers}
\label{sec:boltzmann}

As mentioned above, Planck 2018 provides precise constraints that any cosmology should follow. To connect these observations with theory we must carry out a perturbed general relativistic treatment of how fluctuations in the early universe evolve with its expansion. As emphasized by Weinberg~\cite{weinberg2008}, the observed anisotropies in the CMB arise from several key physical effects:

\begin{itemize}
    \item Intrinsic temperature fluctuations in the photon–baryon plasma at last scattering ($z \simeq 1090$).
    \item Doppler effect from velocity perturbations of the plasma at recombination.
    \item Sachs–Wolfe effect: gravitational redshift or blueshift due to potential fluctuations at last scattering.
    \item Integrated Sachs–Wolfe effect: additional redshifts or blueshifts caused by time-varying gravitational potentials along the photon’s path from last scattering to today.
\end{itemize}

After a full treatment of these effects we arrive at the Einstein–Boltzmann equations, which must be solved for all species (photons, baryons, dark matter, neutrinos, dark energy) in order to accurately predict the CMB angular power spectrum. From linear theory we have that $C_\ell^{XY}$ ($X,Y \in \{T,E,B\}$) are given by:

\begin{equation}
C_\ell^{XY} = 4\pi \int_0^\infty \frac{dk}{k}\,
\mathcal{P}_{\mathcal{R}}(k)\, \Delta_\ell^X(k)\, \Delta_\ell^Y(k),
\end{equation}

where $\mathcal{P}_{\mathcal{R}}(k)$ is the primordial curvature power spectrum and 
$\Delta_\ell^X(k)$ are radiation transfer functions. These transfer functions are computed through the line-of-sight integral~\cite{seljak1996}:

\begin{equation}
\Delta_\ell^X(k) = \int_0^{\eta_0} d\eta \;
S_X(k,\eta)\, j_\ell\!\big(k[\eta_0-\eta]\big),
\end{equation}

with $j_\ell$ spherical Bessel functions and $S_X(k,\eta)$ the source functions encoding Sachs–Wolfe, Doppler, polarization, integrated Sachs–Wolfe, reionization, and lensing effects. 

% FIGURE placeholder: schematic of Einstein–Boltzmann hierarchy

Computing these source functions requires solving the full Einstein–Boltzmann hierarchy. For photons this takes the form

\begin{equation}
\dot\Theta_\ell = \frac{k}{2\ell+1}\big[\ell\,\Theta_{\ell-1} - (\ell+1)\,\Theta_{\ell+1}\big]
- \dot\tau\left(\Theta_\ell - \delta_{\ell 0}\Theta_0^{(\text{src})} - \delta_{\ell 1} v_b - \cdots\right),
\end{equation}

with $\dot\tau$ the Thomson scattering rate and $v_b$ the baryon velocity, coupled in turn to baryons, CDM, neutrinos, and metric perturbations. 

From the sctrcture of (2.9) alone we can already observe that computing the CMB power spectrum can be computationally expensive: each multipole $\Theta_\ell$ is coupled to its neighbors $\Theta_{\ell-1}$ and $\Theta_{\ell+1}$, producing a large system of coupled differential equations. To obtain accurate predictions up to small angular scales, the hierarchy must be evolved up to $\ell \sim 3000$, corresponding to thousands of coupled equations that must be solved for every Fourier mode $k$. 

To tackle this challenge, efficient \textbf{Boltzmann solvers} have been developed. Most notably these include \texttt{CAMB}~\cite{lewis2000} and \texttt{CLASS}~\cite{lesgourgues2011}. 
However, although these solvers are highly efficient and provide accurate theoretical predictions for comparison with Planck, the scale of the computation still means that each run can take seconds. 
While this compute time is fast enough for many applications, it is not suitable for fast \textbf{real-time applications}, thus motivating the creation of a \textbf{surrogate model}. 


\subsection{Surrogate Models}
\label{sec:surrogates}

As the fields of Data Analysis and Machine Learning evolve their impact extends across all areas of science, cosmology is being no exception. A prominent example of this influence is the adoption \textit{surrogate models}.

In this project, we focus on the use of neural networks (NN) as effective surrogates for predicting the temperature power spectrum of the CMB. 
However, much of the existing literature employing such techniques assumes that the reader is already familiar with their underlying principles. 
As a result, neural networks are often presented as opaque ‘black boxes,’ which can leave many researchers either unconvinced by their reliability or uninspired to engage with their use. In the next few sections of this paper we focus on building up these principles and answering why they make highly effective surrogates. 

\subsubsection{The Mathematics Behind Neural Networks}

A neural network can be thought of as a parameterized function $$f_\theta : A \to B$$ 
where $A$ denotes the space of inputs, $B$ the space of outputs, and $\theta$ the collection of learnable parameters. 
The idea is that neural networks approximate complex, often nonlinear, mappings by composing simple transformations.

$$
\text{(Two-step)}\quad
z_j = w_j h_{j-1} + b_j,\qquad h_j = \sigma_j(z_j),
$$
or equivalently

$$
\text{(One-line)}\quad
h_j = \sigma_j\!\big(w_j h_{j-1} + b_j\big),\qquad h_0 = x,
$$
with $w_j,b_j\in\mathbb{R}$. The network output is $f_\theta(x):=h_L$. In composed form, 

$$
f_\theta(x)=\sigma_L\!\Big(w_L\,\sigma_{L-1}\big(\cdots\,\sigma_1(w_1 x + b_1)\,\cdots\big)+b_L\Big).
$$
The parameters $\theta=\{(w_j,b_j)\}_{j=1}^L$ are initialized (typically randomly) and then trained on a dataset

$$
\mathcal{D}=\{(x^{(i)},y^{(i)})\}_{i=1}^N \subset \mathbb{R}\times\mathbb{R},
$$  
Where, $x^{(i)}$ denotes the $i$-th input sample, while $y^{(i)}$ is its corresponding output according to the mapping we aim to approximate. Thus the collection $\{x^{(i)}\}_{i=1}^N$ forms the set of inputs, and $\{y^{(i)}\}_{i=1}^N$forms the set of outputs otherwise known as \textbf{labels}.  

Now let $\mathcal{L}: \mathbb{R} \times \mathbb{R} \to [0, \infty)$ be an arbitrary \textbf{loss function} which measures the discrepancy between a predicted output $\hat y=f_\theta(x)$ and the true output $y$ (for example $\mathcal{L}(\hat y,y)=\tfrac12(\hat y-y)^2$). The average discrepancy over the training set is called \textbf{empirical risk}.

$$
J(\theta) \;=\; \frac{1}{N}\sum_{i=1}^N \mathcal{L}\!\big(f_\theta(x^{(i)}),\,y^{(i)}\big),
  \qquad \theta=\{(w_j,b_j)\}_{j=1}^L
$$
The objective during training is to find parameters that minimize $J(\theta)$. To find such set of parameters the network undergoes the \textbf{gradient descent}. Note that the gradient of the empirical risk with respect to the parameters is defined as: 


$$
\begin{aligned}
  w_j &\;\rightarrow\; w_j - \eta\,\frac{\partial J}{\partial w_j}, \quad j=1, \dots,L, 
  \\
  b_j &\;\rightarrow\; b_j - \eta\,\frac{\partial J}{\partial b_j}, \quad j=1, \dots,L,
  \\
\end{aligned}
$$

In practice we compute $\nabla_\theta J$ via \textbf{backpropagation}. Each step performs a forward pass to form predictions, computes the empirical risk against labels, backpropagates gradients, and updates parameters; an \textbf{epoch} is one full pass over the dataset, and training typically runs for multiple epochs.

The general case replaces scalars with vectors and matrices. For a layer of width $d_j$, let $h_{j-1}\in\mathbb{R}^{d_{j-1}}$, $W_j\in\mathbb{R}^{d_j\times d_{j-1}}$, and $b_j\in\mathbb{R}^{d_j}$. The forward map is

\begin{equation}
  h_j = \sigma_j\!\left(W_j h_{j-1} + b_j\right), 
  \qquad j = 1, \dots , L ,
\end{equation}
where $\sigma_j$ acts elementwise, and training, losses, and gradien-based optimization proceed exactly as in the scalar case.

The true potential of neural networks as surrogates directly stems from this formulation: once trained, the neural network is capable of reproducing our desired mapping using element wise nonlinearactivations and basic matrix multiplication.
Since these operations are highly optimized on modern hardware, evaluation using a NN compared to running full Bolztmann solvers becomes orders of magnitude faster. In theory, allowing for real-time predictions of the CMB power spectrum.

\subsubsection{Training parameters relevant to this project}

\textbf{Depth and Width}: How many layers and how many neurons per layer in the model. More complexity will capture better dynamics but comes with the risk of overfitting.  

\textbf{Activation Functions}: Non-linear functions applied after each linear layer, they let the model learn curved relationships instead of just straight lines. Common choices include: 

- $\mathrm{ReLU}(x)=\max(0,x)$. 
- $\tanh$: Outputs in $(-1,1)$

\textbf{Batch Size}: How many examples you process before taking one optimization step.

\textbf{Learning Rate}: In the update step the scalar $\eta > 0$ controls the step size in parameter space. Too large → divergence; too small → slow, possibly stuck. Common practice: start with $\eta\in[10^{-4},10^{-2}]$

\textbf{Learning Rate Scheduler}: Schedules change the value of the learning rate across epochs to avoid divergence or plateuing. Some schedulers might change according to some function for instance cosine or exponential. 

\subsubsection{PCA}

In practice, we could train a neural network to, given a set of inputs, output a range of $x$ values for the mapping we are trying to predict. 
However, this approach comes with the cost of quickly becoming inefficient or even inaccurate when the output space is high-dimensional. 
In the case of the CMB power spectrum, for instance, a network would have to predict hundreds if not thousands of multipoles simultaneously. While possible, this increases the complexity of the task and the risk of overfitting. 
To reduce the dimensionality of the network output we apply \textbf{Principal Component Analysis (PCA)} to the spectra.

PCA reduces the number of dimensions in large datasets by compressing the data to uncorrelated linear combinations of \textbf{principal components}. 
These components are ordered by the amount of varience they capture, such that the first few components capture the dominant dynamics of variation in the data. 
By projecting our spectra onto the leading $K$ components, we obtain a compact representation that preserves most of the information.
This reduces the dimension of the network output significantly allowing it to learn $K$ coefficients instead of the full high-dimensional spectrum. 

To construct PCA components we first center the dataset $X \in \mathbb{R}^{N \times d}$ by subtracting the mean $\mu$.
We then compute the covariance matrix 
\[
C = \frac{1}{N} (X - \mu)^\top (X - \mu),
\]
and extract its eigenvectors $\{v_k\}$, which form the orthogonal \textbf{principal components}. 
Projecting the data onto these directions yields the coefficients
\[
\alpha = (X - \mu)V,
\]
where $V$ is the matrix whose columns are the top $K$ eigenvectors. To reconstruct the data, we take
\[
\hat{X} = \alpha V^\top + \mu,
\]
which recovers an approximation of the original dataset using only the leading $K$ components.

\section{methodology}
\label{sec:methods}


We constructed a CAMB-only, flat universe dataset by combining three Latin-hypercube-samplers consisting of Planck-like data (CORE), broad data (FULL), and an EDGE band biased to small $\Omega_{\Lambda} h^2$. 
For each sample vector we used CAMB to compute the unlensed TT power spectra up to $\ell_{max}$ and stored it as $\log D_\ell$, then cleaned and merged splits. 
We compressed spectra with PCA to $K \in {20, 50}$ components and trained an MLP to predict \textbf{standardized PCA coefficients} from four density parameters. 
Training used AdamW with cosine annealing, gradient clipping and early stopping. 
Evaluation reconstructs spectra from predicted coefficients and from it we report fractional-error metrics, peak shifts and percentile summaries on a withheld validation set. 

\subsection{Generating the dataset}
\label{sec:data-gen}

\subsubsection{Sampling, Physicality amd Spectra Generation}
\label{sec:data-gen}

To create the dataset we first draw parameter quadruples $x=(\Omega_b h^2,\Omega_{\mathrm{cdm}} h^2,\Omega_\gamma h^2,\Omega_{\mathrm{ur}} h^2)$ 
from three Latin-Hypercube Samplers \footnote{Latin–Hypercube Sampling ensures that each parameter dimension is divided into equiprobable intervals, with one sample drawn from each, thereby providing uniform coverage of the multidimensional parameter space than simple random sampling.}: \textsc{Core} (Planck-like box), \textsc{Full} (broad coverage), and \textbf{EDGE} uses the same box as FULL but selects candidates from the lowest 10\% in $\Omega_\Lambda h^2$ (quantile filter).

\begin{table}[t]
\centering
\small
\caption{Latin–Hypercube samplers: target cardinalities and parameter ranges.} 
\label{tab:lhs-boxes}
\begin{tabular}{l c c c c c}
\toprule
\textbf{Split} & \textbf{Samples} 
& $\boldsymbol{\Omega_b h^2}$ 
& $\boldsymbol{\Omega_{\mathrm{cdm}} h^2}$ 
& $\boldsymbol{\Omega_\gamma h^2}$ 
& $\boldsymbol{\Omega_{\mathrm{ur}} h^2}$ \\
\midrule
CORE & 30{,}000 
& $[\num{0.020},\,\num{0.025}]$
& $[\num{0.105},\,\num{0.135}]$
& $[\num{2.45e-5},\,\num{2.50e-5}]$
& $[\num{1.45e-5},\,\num{1.85e-5}]$ \\
FULL & 15{,}000 
& $[\num{0.010},\,\num{0.030}]$
& $[\num{0.090},\,\num{0.160}]$
& $[\num{2.40e-5},\,\num{2.60e-5}]$
& $[\num{1.20e-5},\,\num{1.90e-5}]$ \\
EDGE$^{\ast}$ & 15{,}000 
& $[\num{0.010},\,\num{0.030}]$
& $[\num{0.090},\,\num{0.160}]$
& $[\num{2.40e-5},\,\num{2.60e-5}]$
& $[\num{1.20e-5},\,\num{1.90e-5}]$ \\
\bottomrule
\end{tabular}

\vspace{2pt}
\raggedright
\footnotesize $^{\ast}$EDGE: candidates are drawn from the FULL box, then filtered to the lowest 10\% in $\Omega_\Lambda h^2$ (edge quantile).
\end{table}

Flatness is imposed by computing:

\[
\Omega_\Lambda h^2 \;=\; h^2-\Omega_b h^2-\Omega_{\mathrm{cdm}} h^2-\Omega_\gamma h^2-\Omega_{\mathrm{ur}} h^2,
\]
and discarding samples with $\Omega_\Lambda<0$. In case a sample is discarded we resample until the target number of samples per split is reached. 

For each accepted $x$ we run \textit{CAMB} (add CAMB parameters in appendix) to obtain the \emph{unlensed} TT power spectrum up to $\ell_{max} = 1000$ and store $d_\ell = \log{D_\ell}$ on a common $\ell$ grid ($\ell=2,\dots,\ell_{\max}$).
All splits are cleaned for shape consistency and merged into a single HDF5 file with parameter columns, $\Omega_\Lambda$, the $\ell$ grid, and $d_\ell$.

\subsubsection{Dimensionality Reduction with PCA} 

From the merged dataset we extract $X \in \mathbb{R}^{N \times L}$ which collects the $L = \ell_{\text{max}} - 1$ values of all $(N)$ $d_\ell$ samples.
Applying PCA with \(K\in\{20,50\}\) to the column–centered \(X\) yields, for each sample, a coefficient vector \(\alpha_i\in\mathbb{R}^K\), together with the mean spectrum \(\mu\in\mathbb{R}^{L}\) and the PCA basis \(B\in\mathbb{R}^{K\times L}\).
The associated eigenvalues $\lambda_k$ quantify the variance explained by each component. 

For training we use standardized scores \(\tilde\alpha_{ik}=\alpha_{ik}/\sqrt{\lambda_k}\); accordingly, 
we write an HDF5 PCA dataset (per \(K\in\{20,50\}\)) containing: \texttt{parameter columns}, \texttt{coefficient vectors},  \texttt{basis} \(B\), \texttt{mean\_spectrum} \(\mu\), \texttt{explained\_variance} \(\lambda\), the \(\ell\)-grid, and the (standardized) \texttt{coefficients} \(\tilde\alpha\) used by the training dataloader. 
Once trained, model-predicted coefficients are mapped back to a log–spectrum via 

\begin{equation}
  \widehat{d} \;=\; \mu \;+\; B^\top
  \big(\widehat{\tilde{\alpha}}\odot\sqrt{\lambda}\big),
  \qquad \widehat{d}\in\mathbb{R}^{L}.
\end{equation}

\subsection{Surrogate Model}
\label{sec:surrogate}

Having defined the inputs (omega densities) and targets (PCA coefficients) we can adopt a fully-connected multilayer perceptron 
$f_\theta:\mathbb{R}^4\to\mathbb{R}^K$ that maps the four density parameters $(\Omega_b h^2, \Omega_{\mathrm{cdm}} h^2, \Omega_\gamma h^2, \Omega_{\mathrm{ur}} h^2)$ 
to the standardized PCA coefficients $\tilde\alpha\in\mathbb{R}^K$. 

The network is composed of $L$ hidden layers of width $d$, each followed by a smooth, learnable activation function.

\begin{equation}
  f(x) \;=\; 
  \Big( \gamma \;+\; \sigma(-\beta \odot x)\,(1-\gamma) \Big) \odot x,
  \qquad 
  \sigma(z) = \frac{1}{1+e^{-z}} ,
\end{equation}


This activation is parameterized by $(\beta, \gamma)$ per layer and interpolates between linear unit and a gated nonlinearity, 
hence allowing the network to adaptively choose the degree of non-linearity per layer.  
The final output ensures that the predicted coefficients span  $\mathbb{R}^K$. 


\subsubsection{Training}

We train our MLP on standardized coefficients with mean squared error.

\[
\mathcal{L}(\theta)=\frac{1}{NK}\sum_{i=1}^N\sum_{k=1}^K 
\left(f_\theta(x^{(i)})_k-\tilde\alpha^{(i)}_k\right)^2.
\]

We optimize this loss using the AdamW optimizer with a cosine–annealed learning rate schedule. 
To stabilize training we apply gradient clipping and monitor the validation loss%
\footnote{During training the dataset is split into a training set, used to update the weights, and a validation set, 
used only for monitoring generalization and triggering early stopping. 
A separate test set is held out entirely and used exclusively for final evaluation.} 
for early stopping. 

To identify the most effective configuration for our MLP we conducted a systematic hyperparameter sweep.
The sweep explores discrete sets of candidate values for key training parameters such as: learning rate,
batch size, hidden layer width, and number of layers. 
Each configuration is then trained with identical conditions (same dataset, loss and optimizer) and evaluated on 
the test set using the metrics defined in \cref{sec:metrics}.


\begin{table}[H]
\centering
\caption{Hyperparameters and candidate values explored in the sweep (from \texttt{hyperparam\_5.yaml}).}
\label{tab:sweep}
\begin{tabular}{lc}
\toprule
\textbf{Hyperparameter} & \textbf{Candidate Values} \\
\midrule
Learning rate & $\{8\times 10^{-4},\,1.2\times 10^{-3},\,1.6\times 10^{-3},\,2.0\times 10^{-3}\}$ \\
Batch size & $\{16,\,32\}$ \\
Hidden layer width ($d$) & $\{64,\,128,\,192,\,256\}$ \\
Number of hidden layers ($L$) & $\{2,\,3,\,4\}$ \\
Weight decay & $\{1\times 10^{-6},\,3\times 10^{-6},\,1\times 10^{-5},\,3\times 10^{-5}\}$ \\
Gradient clipping norm & $\{0.0,\,1.0\}$ \\
Cosine annealing minimum LR & $\{1\times 10^{-5},\,3\times 10^{-5}\}$ \\
Early stopping patience & $\{12,\,16,\,20\}$ \\
Epochs & $100$ (fixed) \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Testing and Evaluation}
\label{sec:metrics}

Once trainins is complete, the surrogate network is evaluated on a \textbf{held–out test set} 
consisting of parameter samples and corresponding spectra generated with \texttt{CAMB}. 
This dataset is entirely disjoint from both the training and validation sets, ensuring an unbiased measure of generalization.

In line with \textsc{CosmoPower}~\cite{SpurioMancini2021}, 
We evaluate the surrogate performance by directly reconstructing spectra from predicted PCA components.
We report \textbf{absolute percentage errors}. For each multipole $\ell$ we compute

\[
|\epsilon_\ell| = 100 \cdot \left| \frac{\widehat{C}_\ell - C_\ell}{C_\ell} \right|,
\]

We then summarize $\{|\epsilon_\ell|\}$ using the mean, median, 95th percentile (p95), 99th percentile (p99), and maximum error.

\subsubsection{Parameter–sensitivity Validation}

Beyond per-multipole error summaries, we asses the surrogate's usability in parameter inference.
First, we generate maps of error statistics as functions of cosmological parameters,
thereby visualizing where in parameter space the surrogate is most accurate.

Second, we perform end-to-end parameter recovery tests: synthetic with \texttt{CAMB} 
are analysed with \texttt{CAMB} an MCMC pipeline using the surrogate in place of \texttt{CAMB}. 
The results are then compared against the ground-truth

Finally we compute Fisher information matrices using both \texttt{CAMB} and the surrogate and veryfy constistency
in predicted parameter uncertainties. These tests are not only a measurement of pointwise accuracy of the surrogate but also 
are a test of the reliability of the model as a useful real-time cosmological parameter estimator. 



\section{Results}
\label{sec:results}
\subsection{Evaluation on Test Set}

We compute three test datasets of size $10^4$ each (CORE, FULL EDGE), sampled using LHS over the parameter boxes Table~\ref{tab:lhs-boxes}
and subject to the same Flatness constraints.
For every test parameter vector, the surrogate predicts PCA components that are mapped into $D_\ell$ for comparison
the corresponding \texttt{CAMB} spectra produced with the same parameters.
Accuracy is evaluated as described in \cref{sec:metrics}.

For each test spectrum $i$ we first form per–multipole absolute percentage errors
$|\epsilon_{i,\ell}| = 100\,|\widehat{C}_{i,\ell} - C_{i,\ell}|/C_{i,\ell}|$
and summarize across $\ell$ to obtain the tuple

\[
s_i \;=\; \big(\mathrm{mean}_\ell\,|\epsilon_{i,\ell}|,\;
              \mathrm{median}_\ell\,|\epsilon_{i,\ell}|,\;
              \mathrm{p95}_\ell\,|\epsilon_{i,\ell}|,\;
              \mathrm{p99}_\ell\,|\epsilon_{i,\ell}|,\;
              \max_\ell\,|\epsilon_{i,\ell}|\big).
\]

Given the collection $\{s_i\}_{i=1}^{N}$ we report accross-dataset summaries by taking, for each component of $s_i$, the median across spectra.
Thus an entry such as "Mean" in Table~\ref{tab:test-summary} corresponds to the median accross the mean absolute error of all test vectors. 
 
\begin{table}[t]
\centering
\small
\caption{Summary of surrogate errors (absolute percentage error $|\epsilon_\ell|$) on the three held--out test sets (10k samples each).
We report the mean, median, 95th percentile (p95), 99th percentile (p99), and maximum error across all multipoles and samples, 
for models trained on $K=20$ and $K=50$ PCA components.}
\label{tab:test-summary}
\begin{tabular}{lcccccc}
\toprule
\textbf{Test Set} & \textbf{Model} 
& \textbf{Mean} & \textbf{Median} & \textbf{p95} & \textbf{p99} & \textbf{Max} \\
\midrule
CORE & PCA--20 & -- & -- & -- & -- & -- \\
     & PCA--50 & -- & -- & -- & -- & -- \\
\midrule
FULL & PCA--20 & -- & -- & -- & -- & -- \\
     & PCA--50 & -- & -- & -- & -- & -- \\
\midrule
EDGE & PCA--20 & -- & -- & -- & -- & -- \\
     & PCA--50 & -- & -- & -- & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

(Add examples with error of both models with samples from three datasets)

\subsection{MCMC parameter inference}
\label{sec:mcmc}

To asses scientific usability beyond pointwise error,
we replace the Boltzmann solver with the surrogate in 
a standard Bayesian pipeline and test recovery of ground 
truth cosmological parameters.

From our held--out set we select the sample whose $(\Omega_b h^2,\Omega_{\rm cdm} h^2,\Omega_\gamma h^2,\Omega_{\rm ur} h^2)$ is closest (in Euclidean norm) to a Planck--like vector $\theta_\star$. Its TT spectrum $D_\ell^{\rm obs}$ (with $\ell=2\!:\!1000$) plays the role of the observed sky.

Given a parameter proposal $\theta=(\Omega_b h^2,\Omega_{\rm cdm} h^2,\Omega_\gamma h^2,\Omega_{\rm ur} h^2)$, , the network predicts PCA coefficients, reconstructs the log-spectrum, and returns $D_\ell(\theta)$ for likelihood evaluation.

We adopt a Gaussian bandpower likelihood with cosmic--variance covariance (TT--only; $f_{\rm sky}=0.6$; no instrument noise by default):
\[
\mathrm{Var}(D_\ell)=\frac{2}{(2\ell+1)f_{\rm sky}}\big(D_\ell+N_\ell^D\big)^2,\qquad N_\ell^D=0,
\]
and flat box priors on each parameter spanning the dataset ranges (with a small safety margin). The log--posterior is
\[
\ln p(\theta\mid D^{\rm obs}) \;=\; -\tfrac12\sum_\ell
\left[\frac{(D_\ell^{\rm obs}-D_\ell(\theta))^2}{\mathrm{Var}(D_\ell)}+\ln\!\big(2\pi\,\mathrm{Var}(D_\ell)\big)\right]
\;+\;\ln p(\theta).
\]
We run MCMC with (\texttt{emcee}), initializing walkers in a small Gaussian ball around $\theta_\star$ and thinning after burn-in.

\section{Discussion}
Discuss implications. For cosmology, see e.g. \citep{planck2018parameters,camb}.

\section{Conclusion}
Summarize contributions and future work.

\paragraph{Reproducibility Checklist}
Version your code/data, fix random seeds, log dependencies, and provide training details.

\section*{Acknowledgments}
Thank funding sources and collaborators here.

% --- References
% If you prefer biblatex + biber, switch packages; here we use BibTeX via natbib.
\bibliography{references}

\appendix
\section{Additional Material}
Extra proofs, ablations, or extended background.

% ==========================
% Build Tips
% ==========================
% FASTEST PATH (BibTeX):
%   pdflatex main
%   bibtex   main
%   pdflatex main
%   pdflatex main
%
% ONE-COMMAND (latexmk, auto-runs bibtex):
%   latexmk -pdf main.tex
%
% If you uncomment \usepackage{minted}, compile with:
%   pdflatex -shell-escape main
%   bibtex   main
%   pdflatex -shell-escape main
%   pdflatex -shell-escape main

\end{document}